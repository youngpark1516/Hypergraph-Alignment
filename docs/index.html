<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Wasserstein Hypergraph Learning for 3D Point Cloud Alignment</title>
  <meta name="description" content="Public-facing project website for Wasserstein Hypergraph Learning for General 3D Point Cloud Alignment." />
  <link rel="stylesheet" href="assets/css/style.css" />
</head>

<body>
  <header class="site-header">
    <nav class="nav">
      <a href="#top" class="brand">WHNN Alignment</a>
      <div class="nav-links">
        <a href="#overview">Overview</a>
        <a href="#problem">Problem</a>
        <a href="#motivation">Motivation</a>
        <a href="#baselines">Baselines</a>
        <a href="#pipeline">Pipeline</a>
        <a href="#evaluation">Evaluation</a>
        <a href="#results">Results</a>
        <a href="#links">Links</a>
      </div>
    </nav>
  </header>

  <main id="top" class="container">
    <!-- HERO -->
    <section class="hero">
      <h1>Wasserstein Hypergraph Learning for General 3D Point Cloud Alignment</h1>

      <div class="people">
        <div class="people-block">
            <div class="people-title">Students</div>
            <div class="people-list">
            <span>Chanyoung Park (chp026@ucsd.edu)</span>
            <span>·</span>
            <span>Minghan Wu (miw039@ucsd.edu)</span>
            </div>
        </div>

        <div class="people-block">
            <div class="people-title">Mentors</div>
            <div class="people-list">
            <span>Gal Mishne (gmishne@ucsd.edu)</span>
            <span>·</span>
            <span>Yusu Wang (yuw122@ucsd.edu)</span>
            </div>
        </div>
        </div>
      <div class="cta-row">
        <a class="btn" target="_blank" rel="noreferrer"
           href="https://github.com/youngpark1516/Hypergraph-Alignment">Code</a>
        <a class="btn btn-outline" target="_blank" rel="noreferrer"
           href="#">Report (PDF)</a>
      </div>
    </section>

    <!-- OVERVIEW -->
    <section id="overview" class="section">
      <h2>Overview</h2>
      <p>
        Accurate 3D point cloud alignment is used for shape correspondence, reconstruction,
        and tracking. In practice, alignment is challenging due to non-rigid deformation,
        partial observations, and noise.
      </p>
      <p>
        This project studies one key design choice in hypergraph alignment pipelines:
        how to aggregate neighborhood (hyperedge) information. Instead of mean pooling,
        we use a Wasserstein-based aggregation that is more sensitive to the distribution
        of features within a neighborhood.
      </p>

      <h3 class="subhead">What we contribute</h3>
      <ul class="takeaways">
        <li>
          <strong>Wasserstein aggregation in hypergraph layers:</strong>
          Replace mean pooling with optimal-transport-inspired aggregation to better preserve
          multi-modal neighborhood structure.
        </li>
        <li>
          <strong>Placement study:</strong>
          Compare where this aggregation is most effective (early vs. late vs. multiple layers).
        </li>
        <li>
          <strong>Alternative hypothesis generation:</strong>
          Implement a matching-based hypothesis generation step as an alternative to ICP for
          piecewise-rigid alignment settings.
        </li>
      </ul>
    </section>

    <!-- PROBLEM SETUP -->
    <section id="problem" class="section">
      <h2>Problem setup</h2>
      <p>
        Given a source point cloud and a target point cloud, our goal is to predict
        correct point-to-point correspondences between the two shapes (dense or semi-dense),
        even when the data includes non-rigid deformation, partial overlap, and noise.
      </p>
      <p>
        We frame this as a classification problem over candidate source–target pairs:
        each candidate pair is predicted as an inlier (true correspondence) or outlier.
        During evaluation, candidates are generated using k-nearest neighbors in feature space.
      </p>
    </section>

    <!-- WHY THIS MATTERS -->
    <section id="motivation" class="section">
      <h2>Why this matters</h2>

      <p>
        Many alignment pipelines rely on local neighborhoods to decide whether a candidate
        correspondence is geometrically consistent. If neighborhood features are summarized poorly,
        the model can confuse true matches with outliers, especially under deformation, partial overlap,
        and noise.
      </p>

      <p>
        A common choice in learning-based hypergraph alignment (including standard HyperGCT) is to use
        mean pooling to summarize hyperedge information. Mean pooling is simple and efficient, but it can
        blur multi-modal neighborhoods and wash out small but important structures. This motivates using a
        distribution-aware aggregation that better preserves how features are spread within a neighborhood.
      </p>

      <details class="details">
        <summary>Optional: technical context (math)</summary>
        <div class="details-body">
          <p>
            Let a hyperedge (neighborhood) contain feature vectors
            <code>{h1, h2, ..., hk}</code> with <code>hi ∈ R^d</code>.
            Standard HyperGCT-style aggregation uses mean pooling:
          </p>

          <pre><code>m_mean = (1/k) * Σ_i hi</code></pre>

          <p>
            Mean pooling compresses the entire set into one vector, but different neighborhoods
            can share the same mean even if their feature distributions are very different.
            To keep distributional information, we treat the neighborhood as an empirical measure:
          </p>

          <pre><code>μ = (1/k) * Σ_i δ(hi)</code></pre>

          <p>
            Our idea is to aggregate by comparing this distribution to a learned reference
            distribution <code>ν</code> using an optimal transport (Wasserstein) distance.
            A common practical approximation is the sliced Wasserstein distance, which projects
            features onto 1D directions <code>θ</code> and averages 1D Wasserstein distances:
          </p>

          <pre><code>SW(μ, ν) = (1/L) * Σ_{ℓ=1..L}  W1( P_{θℓ}# μ , P_{θℓ}# ν )</code></pre>

          <p>
            where <code>P_{θ}#</code> denotes the pushforward distribution after projection onto
            direction <code>θ</code>, and <code>W1</code> is the 1D Wasserstein-1 distance computed
            from sorted projected samples. Intuitively, mean pooling keeps only the first moment,
            while Wasserstein-style aggregation is sensitive to how mass is arranged, helping
            preserve multi-modal neighborhood structure.
          </p>
        </div>
      </details>
    </section>

    <!-- BASELINES -->
    <section id="baselines" class="section">
      <h2>Baselines</h2>

      <div class="bullets">
        <div class="card">
          <h3>ICP (Iterative Closest Point)</h3>
          <p>
            ICP is a classical alignment method that alternates between assigning closest-point
            correspondences and updating the transformation. It works well for rigid or near-rigid
            cases but can be sensitive to initialization and can struggle under partial overlap,
            noise, and non-rigid deformation.
          </p>
        </div>

        <div class="card">
          <h3>HyperGCT (dynamic hypergraph alignment baseline)</h3>
          <p>
            HyperGCT builds a dynamic hypergraph over candidate correspondences and uses hypergraph
            message passing to classify inliers vs. outliers. In the standard HyperGCT pipeline,
            neighborhood (hyperedge) information is summarized using mean pooling. Our method keeps
            the same overall structure and replaces this mean pooling step with Wasserstein-based
            aggregation.
          </p>
        </div>
      </div>
    </section>

    <!-- PIPELINE -->
    <section id="pipeline" class="section">
      <h2>Pipeline</h2>

      <p>
        Our system follows the overall structure of a dynamic hypergraph alignment baseline (HyperGCT):
        we build a hypergraph over candidate correspondences and refine them through hypergraph message passing.
        Our main change is replacing mean pooling inside hypergraph layers with Wasserstein-based aggregation.
      </p>

      <ol class="pipeline">
        <li>
          <strong>Generate candidate correspondences:</strong>
          form source–target pairs (train: labeled positives/negatives; eval: kNN candidates).
        </li>
        <li>
          <strong>Build a dynamic hypergraph:</strong>
          treat each candidate pair as a node and group nodes into hyperedges based on geometric consistency.
        </li>
        <li>
          <strong>Hypergraph message passing:</strong>
          update node features and predict an inlier confidence score per candidate.
        </li>
        <li>
          <strong>Wasserstein aggregation (our focus):</strong>
          replace mean pooling with a distribution-aware aggregation in selected layers
          (we compare different insertion points).
        </li>
        <li>
          <strong>Hypothesis generation:</strong>
          use predicted inliers to produce an alignment hypothesis (matching-based alternative to ICP).
        </li>
      </ol>

      <details class="details">
        <summary>What exactly changed vs. baseline?</summary>
        <div class="details-body">
          <ul>
            <li>Baseline: mean pooling to summarize features within each hyperedge/neighborhood.</li>
            <li>Ours: Wasserstein-based aggregation to preserve multi-modal structure in neighborhoods.</li>
            <li>We study early vs. late vs. multi-layer insertion of the aggregation module.</li>
          </ul>
        </div>
      </details>
    </section>
    
    <!-- EVALUATION -->
    <section id="evaluation" class="section">
      <h2>Evaluation</h2>

      <p>
        We evaluate correspondence prediction as a classification problem over candidate
        source–target pairs. At evaluation time, the candidate set is built using label-blind
        feature k-nearest-neighbor (kNN) retrieval between the two point clouds.
      </p>

      <div class="bullets">
        <div class="card">
          <h3>Datasets & perturbations</h3>
          <p>
            Experiments are conducted on FAUST and PartNet under controlled perturbations.
            We evaluate rigid and non-rigid settings, including affine / piecewise-rigid regimes.
            For affine cases, distortion magnitude is controlled by a scalar deformation/noise
            level σ (e.g., σ = 0 and ranges such as σ ∈ [0, 1.5] and σ ∈ [0, 2]).
          </p>
        </div>

        <div class="card">
          <h3>Task metrics (pair classification)</h3>
          <p>
            We report standard correspondence classification metrics: precision, recall, and F1,
            along with training losses (classification loss, graph loss, and matching MSE).
          </p>
        </div>

        <div class="card">
          <h3>After hypothesis generation</h3>
          <p>
            After converting predicted confidences into a matching plan, we evaluate with a
            distance-aware metric on the resulting correspondences. In our pipeline, greedy
            and two-stage spectral matching behave similarly and both outperform global spectral,
            so we use two-stage spectral matching to compute matching plans.
          </p>
        </div>

        <div class="card">
          <h3>Structural diagnostics (hypergraph quality)</h3>
          <p>
            Beyond pair accuracy, we track diagnostics that reflect the learned hypergraph structure:
            aggregatedness, disjointness, and in-edge distance consistency (variance of point distances
            among pairs within the same hyperedge). These help interpret robustness and failure modes.
          </p>
        </div>
      </div>

      <details class="details">
        <summary>Optional: metric definitions (math)</summary>
        <div class="details-body">
          <p>
            Precision = TP / (TP + FP), Recall = TP / (TP + FN),
            and F1 = 2PR / (P + R).
          </p>
          <p>
            For distance-aware evaluation, we also measure how far predicted correspondences deviate
            from ground-truth target locations over the subset of pairs predicted as true.
          </p>
        </div>
      </details>
    </section>

    <!-- RESULTS -->
    <section id="results" class="section">
      <h2>Results & takeaways</h2>
      <p>
        Overall, our results indicate that Wasserstein-based aggregation can improve performance
        over mean pooling and can change robustness behavior across perturbation regimes.
      </p>

      <ul class="takeaways">
        <li><strong>Takeaway 1:</strong> Distribution-aware aggregation can preserve local structure that averaging loses.</li>
        <li><strong>Takeaway 2:</strong> Where you insert Wasserstein aggregation (early vs. late) can matter.</li>
        <li><strong>Takeaway 3:</strong> Robustness behavior can shift across perturbation settings.</li>
      </ul>

      <details class="details">
        <summary>Future: add 1–2 key plots/tables</summary>
        <div class="details-body">
          <p>
            (Later) Insert a small number of figures that tell the story at a glance.
            Keep it skimmable.
          </p>
        </div>
      </details>
    </section>

    <!-- LINKS -->
    <section id="links" class="section">
      <h2>Links</h2>
      <ul class="link-list">
        <li><a target="_blank" rel="noreferrer" href="https://github.com/youngpark1516/Hypergraph-Alignment">Code repository</a></li>
        <li><a target="_blank" rel="noreferrer" href="">Report (PDF)</a></li>
      </ul>
    </section>

    <footer class="footer">
      <p>
        Built as a static site and hosted on GitHub Pages. © <span id="year"></span>
      </p>
    </footer>
  </main>

  <script src="assets/js/main.js"></script>
</body>
</html>