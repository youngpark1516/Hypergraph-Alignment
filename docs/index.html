<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Wasserstein Hypergraph Learning for 3D Point Cloud Alignment</title>
  <meta name="description" content="Public-facing project website for Wasserstein Hypergraph Learning for General 3D Point Cloud Alignment." />
  <link rel="stylesheet" href="assets/css/style.css" />

  <!-- KaTeX (math rendering) -->
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.css" />
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/katex.min.js"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.25/dist/contrib/auto-render.min.js"></script>

  <!-- Optional local styles for math blocks -->
  <style>
    .eq {
      margin: 0.85rem 0;
      overflow-x: auto;
    }
    .eq--inline-note {
      margin-top: 0.4rem;
    }
  </style>
</head>

<body>
  <header class="site-header">
    <nav class="nav">
      <a href="#top" class="brand">WHNN Alignment</a>
      <div class="nav-links">
        <a href="#overview">Overview</a>
        <a href="#problem">Problem</a>
        <a href="#motivation">Motivation</a>
        <a href="#baselines">Baselines</a>
        <a href="#method">Method</a>
        <a href="#evaluation">Evaluation</a>
        <a href="#results">Results</a>
        <a href="#links">Links</a>
      </div>
    </nav>
  </header>

  <main id="top" class="container">
    <!-- HERO -->
    <section class="hero">
      <h1>Wasserstein Hypergraph Learning for General 3D Point Cloud Alignment</h1>

      <div class="people">
        <div class="people-block">
          <div class="people-title">Students</div>
          <div class="people-list">
            <span>Chanyoung Park (chp026@ucsd.edu)</span>
            <span>·</span>
            <span>Minghan Wu (miw039@ucsd.edu)</span>
          </div>
        </div>

        <div class="people-block">
          <div class="people-title">Mentors</div>
          <div class="people-list">
            <span>Gal Mishne (gmishne@ucsd.edu)</span>
            <span>·</span>
            <span>Yusu Wang (yuw122@ucsd.edu)</span>
          </div>
        </div>
      </div>

      <div class="cta-row">
        <a class="btn" target="_blank" rel="noreferrer"
           href="https://github.com/youngpark1516/Hypergraph-Alignment">Code</a>
        <a class="btn btn-outline" target="_blank" rel="noreferrer"
           href="#">Report (PDF)</a>
      </div>
    </section>

    <!-- OVERVIEW -->
    <section id="overview" class="section">
      <h2>Overview</h2>
      <p>
        Accurate 3D point cloud alignment is used for shape correspondence, reconstruction,
        and tracking. In practice, alignment is challenging due to non-rigid deformation,
        partial observations, and noise.
      </p>
      <p>
        This project studies one key design choice in hypergraph alignment pipelines:
        how to aggregate neighborhood (hyperedge) information. Instead of mean pooling,
        we use a Wasserstein-based aggregation that is more sensitive to the distribution
        of features within a neighborhood.
      </p>

      <h3 class="subhead">What we contribute</h3>
      <ul class="takeaways">
        <li>
          <strong>Wasserstein aggregation in hypergraph layers:</strong>
          Replace mean pooling with optimal-transport-inspired aggregation to better preserve
          multi-modal neighborhood structure.
        </li>
        <li>
          <strong>Placement study:</strong>
          Compare where this aggregation is most effective (early vs. late vs. multiple layers).
        </li>
        <li>
          <strong>Alternative hypothesis generation:</strong>
          Implement a matching-based hypothesis generation step as an alternative to ICP for
          piecewise-rigid alignment settings.
        </li>
      </ul>
    </section>

    <!-- PROBLEM SETUP -->
    <section id="problem" class="section">
      <h2>Problem setup</h2>
      <p>
        Given a source point cloud and a target point cloud, our goal is to predict
        correct point-to-point correspondences between the two shapes (dense or semi-dense),
        even when the data includes non-rigid deformation, partial overlap, and noise.
      </p>
      <p>
        We frame this as a classification problem over candidate source–target pairs:
        each candidate pair is predicted as an inlier (true correspondence) or outlier.
        During evaluation, candidates are generated using k-nearest neighbors in feature space.
      </p>
    </section>

    <!-- WHY THIS MATTERS -->
    <section id="motivation" class="section">
      <h2>Why this matters</h2>

      <p>
        Many alignment pipelines rely on local neighborhoods to decide whether a candidate
        correspondence is geometrically consistent. If neighborhood features are summarized poorly,
        the model can confuse true matches with outliers, especially under deformation, partial overlap,
        and noise.
      </p>

      <p>
        A common choice in learning-based hypergraph alignment (including standard HyperGCT) is to use
        mean pooling to summarize hyperedge information. Mean pooling is simple and efficient, but it can
        blur multi-modal neighborhoods and wash out small but important structures. This motivates using a
        distribution-aware aggregation that better preserves how features are spread within a neighborhood.
      </p>

      <details class="details">
        <summary>Technical note</summary>
        <div class="details-body">
          <p>
            Let a hyperedge (neighborhood) contain feature vectors
            \( \{h_1, h_2, \dots, h_k\} \) with \( h_i \in \mathbb{R}^d \).
            Standard HyperGCT-style aggregation uses mean pooling:
          </p>

          <div class="eq">
            \[
              m_{\text{mean}} = \frac{1}{k}\sum_{i=1}^{k} h_i
            \]
          </div>

          <p>
            Mean pooling compresses the entire set into one vector, but different neighborhoods
            can share the same mean even if their feature distributions are very different.
            To keep distributional information, we treat the neighborhood as an empirical measure:
          </p>

          <div class="eq">
            \[
              \mu = \frac{1}{k}\sum_{i=1}^{k} \delta(h_i)
            \]
          </div>

          <p>
            Our idea is to aggregate by comparing this distribution to a learned reference
            distribution \( \nu \) using an optimal transport (Wasserstein) distance.
            A common practical approximation is the sliced Wasserstein distance, which projects
            features onto 1D directions \( \theta \) and averages 1D Wasserstein distances:
          </p>

          <div class="eq">
            \[
              \mathrm{SW}(\mu,\nu)
              =
              \frac{1}{L}\sum_{\ell=1}^{L}
              W_1\!\left((P_{\theta_\ell})_{\#}\mu,\,(P_{\theta_\ell})_{\#}\nu\right)
            \]
          </div>

          <p>
            where \( (P_{\theta})_{\#} \) denotes the pushforward distribution after projection onto
            direction \( \theta \), and \( W_1 \) is the 1D Wasserstein-1 distance computed
            from sorted projected samples. Intuitively, mean pooling keeps only the first moment,
            while Wasserstein-style aggregation is sensitive to how mass is arranged, helping
            preserve multi-modal neighborhood structure.
          </p>
        </div>
      </details>
    </section>

    <!-- BASELINES -->
    <section id="baselines" class="section">
      <h2>Baselines</h2>

      <div class="bullets">
        <div class="card">
          <h3>ICP (Iterative Closest Point)</h3>
          <p>
            ICP is a classical alignment method that alternates between assigning closest-point
            correspondences and updating the transformation. It works well for rigid or near-rigid
            cases but can be sensitive to initialization and can struggle under partial overlap,
            noise, and non-rigid deformation.
          </p>
        </div>

        <div class="card">
          <h3>HyperGCT (dynamic hypergraph alignment baseline)</h3>
          <p>
            HyperGCT builds a dynamic hypergraph over candidate correspondences and uses hypergraph
            message passing to classify inliers vs. outliers. In the standard HyperGCT pipeline,
            neighborhood (hyperedge) information is summarized using mean pooling. Our method keeps
            the same overall structure and replaces this mean pooling step with Wasserstein-based
            aggregation.
          </p>
          <figure class="figure-card">
            <img src="assets/img/hypergct.png" alt="HyperGCT architecture diagram" class="figure-img" />
            <figcaption class="figure-caption">
              HyperGCT pipeline: hypergraph construction, convolution, update blocks, and hypothesis generation.
            </figcaption>
          </figure>
        </div>
      </div>
    </section>

    <!-- METHOD -->
    <section id="method" class="section">
      <h2>Method</h2>

      <p>
        We follow a dynamic hypergraph alignment baseline (HyperGCT): build a hypergraph over candidate
        correspondences and refine them with hypergraph message passing. Our main change replaces mean pooling
        inside selected hypergraph layers with Wasserstein-based aggregation.
      </p>

      <ol class="pipeline">
        <li>
          <strong>Candidate generation:</strong>
          form source–target pairs (train: labeled positives/negatives; eval: kNN candidates).
        </li>
        <li>
          <strong>Dynamic hypergraph construction:</strong>
          treat each candidate pair as a node and group nodes into hyperedges based on geometric consistency.
        </li>
        <li>
          <strong>Hypergraph message passing:</strong>
          update node features and predict an inlier confidence score per candidate.
        </li>
        <li>
          <strong>Wasserstein aggregation:</strong>
          replace mean pooling with distribution-aware aggregation in selected layers (early vs. late vs. multi-layer).
        </li>
        <li>
          <strong>Hypothesis generation:</strong>
          convert inlier scores into a matching plan and estimate an alignment hypothesis (matching-based alternative to ICP).
        </li>
      </ol>

      <details class="details">
        <summary>What changed from the baseline</summary>
        <div class="details-body">
          <ul>
            <li>Baseline: mean pooling within each hyperedge/neighborhood.</li>
            <li>Ours: Wasserstein-based aggregation to preserve multi-modal neighborhood structure.</li>
            <li>Study: compare insertion points (early vs. late vs. multiple layers).</li>
          </ul>
        </div>
      </details>
    </section>

    <!-- EVALUATION -->
    <section id="evaluation" class="section">
      <h2>Evaluation</h2>

      <p>
        We evaluate correspondence prediction as binary classification over candidate source–target pairs.
        At test time, candidates are built using label-blind feature kNN retrieval between the two point clouds.
      </p>

      <div class="bullets">
        <div class="card">
          <h3>Data</h3>
          <ul>
            <li><strong>Datasets:</strong> FAUST, PartNet.</li>
            <li><strong>Settings:</strong> rigid, non-rigid, affine / piecewise-rigid regimes.</li>
            <li><strong>Perturbations:</strong> deformation/noise level controlled by \( \sigma \) (e.g., \( \sigma = 0 \); ranges such as \( \sigma \in [0,1.5] \), \( \sigma \in [0,2] \)).</li>
          </ul>
        </div>

        <div class="card">
          <h3>Protocol</h3>
          <ul>
            <li><strong>Candidate set:</strong> kNN in feature space (label-blind) between source and target.</li>
            <li><strong>Prediction:</strong> model outputs an inlier confidence per candidate pair.</li>
            <li><strong>Matching:</strong> convert confidences into a matching plan via two-stage spectral matching.</li>
          </ul>
        </div>

        <div class="card">
          <h3>Pair metrics</h3>
          <ul>
            <li><strong>Precision / Recall / F1</strong> on candidate inlier classification.</li>
            <li><strong>Training signals:</strong> classification loss, graph loss, matching MSE (reported as curves/summary values).</li>
          </ul>
        </div>

        <div class="card">
          <h3>Matching metrics</h3>
          <ul>
            <li><strong>Distance-aware error:</strong> deviation from ground-truth target locations over predicted matches.</li>
            <li><strong>Hypothesis quality:</strong> evaluate correspondence accuracy after converting scores to a matching plan.</li>
          </ul>
        </div>

        <div class="card">
          <h3>Structure diagnostics</h3>
          <ul>
            <li><strong>Aggregatedness:</strong> how concentrated predictions become within hyperedges.</li>
            <li><strong>Disjointness:</strong> separation between hyperedges / neighborhood overlap behavior.</li>
            <li><strong>In-edge distance consistency:</strong> variance of point distances among pairs within the same hyperedge.</li>
          </ul>
        </div>
      </div>

      <details class="details">
        <summary>Metric definitions</summary>
        <div class="details-body">
          <p>
            <strong>Precision / Recall / F1:</strong>
            \( \mathrm{Precision} = \frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FP}} \),
            \( \mathrm{Recall} = \frac{\mathrm{TP}}{\mathrm{TP}+\mathrm{FN}} \),
            and
            \( \mathrm{F1} = \frac{2PR}{P+R} \).
          </p>

          <p>
            <strong>Aggregatedness:</strong>
            measures how concentrated predicted inlier mass is within hyperedges.
            Let \( s_v \) be the predicted inlier score for node (candidate pair) \( v \).
            For each hyperedge \( e \) with nodes \( V(e) \), define the normalized mass
            \( p_v = \frac{s_v}{\sum_{u \in V(e)} s_u + \varepsilon} \).
            Aggregatedness is the average concentration over hyperedges:
          </p>

          <div class="eq">
            \[
              A = \frac{1}{|E|}\sum_{e\in E}\sum_{v\in V(e)} p_v^2
            \]
          </div>

          <p>
            Higher \( A \) means scores collapse onto a few nodes per hyperedge (more “peaky”);
            lower \( A \) means scores are spread out.
          </p>

          <p>
            <strong>Disjointness:</strong>
            measures how much hyperedges overlap in membership. For hyperedges \( e \) and \( f \),
            define overlap via Jaccard similarity
            \( J(e,f) = \frac{|V(e)\cap V(f)|}{|V(e)\cup V(f)|} \).
            Disjointness is one minus average overlap over sampled/neighboring hyperedge pairs:
          </p>

          <div class="eq">
            \[
              D = 1 - \frac{1}{|P|}\sum_{(e,f)\in P} J(e,f)
            \]
          </div>

          <p>
            Higher \( D \) means hyperedges share fewer nodes (cleaner separation).
          </p>

          <p>
            <strong>In-edge distance consistency:</strong>
            measures geometric consistency within each hyperedge. Each node \( v=(i,j) \)
            corresponds to a source point \( p_i \) and target point \( q_j \).
            For a hyperedge \( e \), compute pairwise source distances
            \( d_{\text{src}}(v,u)=\lVert p_i-p_{i'}\rVert \)
            and target distances
            \( d_{\text{tgt}}(v,u)=\lVert q_j-q_{j'}\rVert \)
            for nodes \( v=(i,j) \), \( u=(i',j') \).
            The consistency error per pair is
            \( \Delta(v,u)=\lvert d_{\text{src}}(v,u)-d_{\text{tgt}}(v,u)\rvert \).
            We report variance within each hyperedge, averaged across hyperedges:
          </p>

          <div class="eq">
            \[
              C = \frac{1}{|E|}\sum_{e\in E}
              \operatorname{Var}_{v,u\in V(e),\,v\neq u}\!\big[\Delta(v,u)\big]
            \]
          </div>

          <p>
            Lower \( C \) means a hyperedge’s correspondences agree more strongly on geometry.
          </p>
        </div>
      </details>
    </section>

    <!-- RESULTS -->
    <section id="results" class="section">
      <h2>Results & takeaways</h2>
      <p>
        Overall, our results indicate that Wasserstein-based aggregation can improve performance
        over mean pooling and can change robustness behavior across perturbation regimes.
      </p>

      <ul class="takeaways">
        <li><strong>Takeaway 1:</strong> Distribution-aware aggregation can preserve local structure that averaging loses.</li>
        <li><strong>Takeaway 2:</strong> Where you insert Wasserstein aggregation (early vs. late) can matter.</li>
        <li><strong>Takeaway 3:</strong> Robustness behavior can shift across perturbation settings.</li>
      </ul>

      <div class="plot-embed" style="margin-top:1.5rem;">
        <iframe
          src="assets/plots/wass_metrics_grid.html"
          style="width:100%;height:520px;border:none;border-radius:6px;"
          title="Validation metrics across Wasserstein aggregation placements"
          loading="lazy"
        ></iframe>
      </div>
    </section>

    <!-- LINKS -->
    <section id="links" class="section">
      <h2>Links</h2>
      <ul class="link-list">
        <li><a target="_blank" rel="noreferrer" href="https://github.com/youngpark1516/Hypergraph-Alignment">Code repository</a></li>
        <li><a target="_blank" rel="noreferrer" href="">Report (PDF)</a></li>
      </ul>
    </section>

    <footer class="footer">
      <p>
        Built as a static site and hosted on GitHub Pages. © <span id="year"></span>
      </p>
    </footer>
  </main>

  <script src="assets/js/main.js"></script>

  <!-- KaTeX auto-render init -->
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      renderMathInElement(document.body, {
        delimiters: [
          { left: "$$", right: "$$", display: true },
          { left: "\\[", right: "\\]", display: true },
          { left: "\\(", right: "\\)", display: false },
          { left: "$", right: "$", display: false }
        ],
        ignoredTags: ["script", "noscript", "style", "textarea", "pre", "code"],
        throwOnError: false
      });
    });
  </script>
</body>
</html>